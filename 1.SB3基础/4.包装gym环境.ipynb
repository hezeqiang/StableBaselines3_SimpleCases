{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([-0.93968606,  0.34203818,  0.8378868 ], dtype=float32), {})"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "import gymnasium as gym\n",
    "\n",
    "#自定义一个Wrapper\n",
    "class Pendulum(gym.Wrapper):\n",
    "\n",
    "    def __init__(self,env):\n",
    "\n",
    "        super().__init__(env)\n",
    "\n",
    "    def reset(self,**kwargs):\n",
    "\n",
    "        return self.env.reset(**kwargs)\n",
    "\n",
    "    def step(self, action):\n",
    "        state, reward, terminated, truncated, info = self.env.step(action)\n",
    "        return state, reward, terminated, truncated, info\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "env = gym.make('Pendulum-v1')\n",
    "\n",
    "env = Pendulum(env)\n",
    "\n",
    "env.reset()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<Pendulum<TimeLimit<OrderEnforcing<PassiveEnvChecker<PendulumEnv<Pendulum-v1>>>>>>\n",
      "0 (array([ 0.7251202, -0.6886223,  0.6319107], dtype=float32), {}) [0.09473597] -0.6169130829046054\n",
      "20 [ 0.06014106  0.99818987 -4.3009367 ] [-1.0366311] -4.132849996695271\n",
      "40 [-0.94985515 -0.3126903   6.765589  ] [-1.76569] -12.552977268837097\n",
      "60 [-0.9994354   0.03359844 -6.3827085 ] [1.9138559] -13.737148499817884\n",
      "80 [-0.9995127  -0.03121652  6.5411453 ] [1.4109446] -13.955056970091478\n",
      "100 [-0.10480989 -0.9944923  -4.628027  ] [1.1031991] -4.951383044248425\n",
      "120 [0.18274213 0.98316085 2.563205  ] [0.23092084] -2.580883952906665\n",
      "140 [ 0.53790003 -0.84300864 -1.1046811 ] [-1.2574584] -1.129325812540723\n",
      "160 [ 0.53609085  0.8441603  -0.50069   ] [1.3799686] -1.0369920930097003\n",
      "180 [ 0.41210884 -0.9111346   1.4649147 ] [1.7209471] -1.5309415195420417\n",
      "200 [ 0.5269767   0.84987974 -3.844894  ] [-0.4150266] -2.510255523893626\n"
     ]
    }
   ],
   "source": [
    "#测试一个环境\n",
    "def test(env, wrap_action_in_list=False):\n",
    "    print(env)\n",
    "\n",
    "    state = env.reset()\n",
    "    over = False\n",
    "    step = 0\n",
    "\n",
    "    while not over:\n",
    "        action = env.action_space.sample()\n",
    "\n",
    "        if wrap_action_in_list:\n",
    "            action = [action]\n",
    "\n",
    "        next_state, reward, terminated, truncated, info = env.step(action)\n",
    "\n",
    "        if step % 20 == 0:\n",
    "            print(step, state, action, reward)\n",
    "\n",
    "        if step > 200:\n",
    "            break\n",
    "\n",
    "        state = next_state\n",
    "        step += 1\n",
    "\n",
    "\n",
    "test(env)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Eb2U4_K6SNUx"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<StepLimitWrapper<Pendulum<TimeLimit<OrderEnforcing<PassiveEnvChecker<PendulumEnv<Pendulum-v1>>>>>>>\n",
      "0 (array([ 0.615082  ,  0.7884631 , -0.31675813], dtype=float32), {}) [-0.588578] -0.8354004206778554\n",
      "20 [ 0.47905335 -0.8777858   2.9411707 ] [0.90504074] -2.0133806712601556\n",
      "40 [-0.6984637   0.71564555 -7.1090546 ] [0.4106314] -10.548580158206635\n",
      "60 [0.10262237 0.9947204  4.372786  ] [-0.6475478] -4.067548470342698\n",
      "80 [ 0.6596029 -0.7516143 -1.0035911] [-0.80827516] -0.8247332677756085\n",
      "100 [ 0.5947022  0.8039461 -1.1527305] [-0.38490862] -1.0051983453082127\n",
      "120 [ 0.2186914 -0.9757941  4.011166 ] [0.8221919] -3.4329938541571146\n",
      "140 [-0.72549313  0.6882294  -6.4105854 ] [0.18571568] -9.786125442605494\n",
      "160 [-0.97686565  0.21385406  6.6089096 ] [-0.78359425] -12.93029208921303\n",
      "180 [-0.78576165 -0.61852944 -6.066613  ] [-0.47249308] -9.804856105694457\n",
      "200 [-0.3480743  0.937467   4.3219748] [-0.3518394] -5.578750330684813\n"
     ]
    }
   ],
   "source": [
    "#修改最大步数\n",
    "class StepLimitWrapper(gym.Wrapper):\n",
    "\n",
    "    def __init__(self, env):\n",
    "        super().__init__(env)\n",
    "        self.current_step = 0\n",
    "\n",
    "    def reset(self):\n",
    "        self.current_step = 0\n",
    "        return self.env.reset()\n",
    "\n",
    "    def step(self, action):\n",
    "        self.current_step += 1\n",
    "        state, reward, terminated, truncated, info = self.env.step(action)\n",
    "\n",
    "        #修改done字段\n",
    "        if self.current_step >= 100:\n",
    "            done = True\n",
    "\n",
    "        return state, reward, terminated, truncated, info\n",
    "\n",
    "\n",
    "test(StepLimitWrapper(env))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "F5E6kZfzW8vy"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<NormalizeActionWrapper<TimeLimit<OrderEnforcing<PassiveEnvChecker<PendulumEnv<Pendulum-v1>>>>>>\n",
      "0 (array([ 0.98162663, -0.19081193,  0.11409011], dtype=float32), {}) [0.50907546] -0.0391981431290179\n",
      "20 [-0.7588267  0.6512926 -7.3505583] [0.32093418] -11.319595516205856\n",
      "40 [-0.48303744  0.8755997   6.1656275 ] [-0.98626465] -8.110666487439769\n",
      "60 [ 0.84920865 -0.5280574  -0.62930065] [0.19350258] -0.34923407237200926\n",
      "80 [ 0.36137825  0.9324193  -2.5541186 ] [-0.39311224] -2.095493200804351\n",
      "100 [-4.7682617e-03 -9.9998862e-01  5.2046952e+00] [-0.503002] -5.192301253861725\n",
      "120 [-0.73664594 -0.6762786  -6.538787  ] [-0.8995183] -10.03350072532808\n",
      "140 [0.3133714 0.9496306 3.1349335] [-0.95959073] -2.554106075833723\n",
      "160 [ 0.6556962  -0.7550248  -0.20849022] [-0.9087635] -0.7398589737489942\n",
      "180 [ 0.526781    0.85000104 -1.834008  ] [-0.51753384] -1.3696600233936655\n",
      "200 [ 0.15338081 -0.98816717  3.634122  ] [-0.571055] -3.3293326184681664\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "\n",
    "#修改动作空间\n",
    "class NormalizeActionWrapper(gym.Wrapper):\n",
    "\n",
    "    def __init__(self, env):\n",
    "        #获取动作空间\n",
    "        action_space = env.action_space\n",
    "\n",
    "        #动作空间必须是连续值\n",
    "        assert isinstance(action_space, gym.spaces.Box)\n",
    "\n",
    "        #重新定义动作空间,在正负一之间的连续值\n",
    "        #这里其实只影响env.action_space.sample的返回结果\n",
    "        #实际在计算时,还是正负2之间计算的\n",
    "        env.action_space = gym.spaces.Box(low=-1,\n",
    "                                          high=1,\n",
    "                                          shape=action_space.shape,\n",
    "                                          dtype=np.float32)\n",
    "\n",
    "        super().__init__(env)\n",
    "\n",
    "    def reset(self):\n",
    "        return self.env.reset()\n",
    "\n",
    "    def step(self, action):\n",
    "        #重新缩放动作的值域\n",
    "        action = action * 2.0\n",
    "\n",
    "        if action > 2.0:\n",
    "            action = 2.0\n",
    "\n",
    "        if action < -2.0:\n",
    "            action = -2.0\n",
    "\n",
    "        return self.env.step(action)\n",
    "\n",
    "env = gym.make('Pendulum-v1')\n",
    "test(NormalizeActionWrapper(env))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "bBlS9YxYSpJn"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<StateStepWrapper<TimeLimit<OrderEnforcing<PassiveEnvChecker<PendulumEnv<Pendulum-v1>>>>>>\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "setting an array element with a sequence. The requested array has an inhomogeneous shape after 1 dimensions. The detected shape was (2,) + inhomogeneous part.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[23], line 45\u001b[0m\n\u001b[0;32m     42\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m np\u001b[38;5;241m.\u001b[39mconcatenate([state, [state_step]])\n\u001b[0;32m     44\u001b[0m env \u001b[38;5;241m=\u001b[39m gym\u001b[38;5;241m.\u001b[39mmake(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mPendulum-v1\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m---> 45\u001b[0m \u001b[43mtest\u001b[49m\u001b[43m(\u001b[49m\u001b[43mStateStepWrapper\u001b[49m\u001b[43m(\u001b[49m\u001b[43menv\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[6], line 5\u001b[0m, in \u001b[0;36mtest\u001b[1;34m(env, wrap_action_in_list)\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mtest\u001b[39m(env, wrap_action_in_list\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m):\n\u001b[0;32m      3\u001b[0m     \u001b[38;5;28mprint\u001b[39m(env)\n\u001b[1;32m----> 5\u001b[0m     state \u001b[38;5;241m=\u001b[39m \u001b[43menv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreset\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      6\u001b[0m     over \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[0;32m      7\u001b[0m     step \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n",
      "Cell \u001b[1;32mIn[23], line 26\u001b[0m, in \u001b[0;36mStateStepWrapper.reset\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     24\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mreset\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m     25\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstep_current \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[1;32m---> 26\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconcatenate\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43menv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreset\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0.0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mValueError\u001b[0m: setting an array element with a sequence. The requested array has an inhomogeneous shape after 1 dimensions. The detected shape was (2,) + inhomogeneous part."
     ]
    }
   ],
   "source": [
    "from gymnasium.wrappers import TimeLimit\n",
    "\n",
    "\n",
    "#修改状态\n",
    "class StateStepWrapper(gym.Wrapper):\n",
    "\n",
    "    def __init__(self, env):\n",
    "\n",
    "        #状态空间必须是连续值\n",
    "        assert isinstance(env.observation_space, gym.spaces.Box)\n",
    "\n",
    "        #增加一个新状态字段\n",
    "        low = np.concatenate([env.observation_space.low, [0.0]])\n",
    "        high = np.concatenate([env.observation_space.high, [1.0]])\n",
    "\n",
    "        env.observation_space = gym.spaces.Box(low=low,\n",
    "                                               high=high,\n",
    "                                               dtype=np.float32)\n",
    "\n",
    "        super().__init__(env)\n",
    "\n",
    "        self.step_current = 0\n",
    "\n",
    "    def reset(self):\n",
    "        self.step_current = 0\n",
    "        return np.concatenate([self.env.reset(), [0.0]])\n",
    "\n",
    "    def step(self, action):\n",
    "        self.step_current += 1\n",
    "        state, reward, done, info = self.env.step(action)\n",
    "\n",
    "        #根据step_max修改done\n",
    "        if self.step_current >= 100:\n",
    "            done = True\n",
    "\n",
    "        return self.get_state(state), reward, done, info\n",
    "\n",
    "    def get_state(self, state):\n",
    "        #添加一个新的state字段\n",
    "        state_step = self.step_current / 100\n",
    "\n",
    "        return np.concatenate([state, [state_step]])\n",
    "\n",
    "env = gym.make('Pendulum-v1')\n",
    "test(StateStepWrapper(env))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "8cxnE5bdaQ_3",
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cuda device\n",
      "-------------------------------------\n",
      "| rollout/              |           |\n",
      "|    ep_len_mean        | 200       |\n",
      "|    ep_rew_mean        | -1.34e+03 |\n",
      "| time/                 |           |\n",
      "|    fps                | 449       |\n",
      "|    iterations         | 100       |\n",
      "|    time_elapsed       | 1         |\n",
      "|    total_timesteps    | 500       |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -1.41     |\n",
      "|    explained_variance | -0.11     |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 99        |\n",
      "|    policy_loss        | -13.6     |\n",
      "|    std                | 0.987     |\n",
      "|    value_loss         | 43.3      |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| rollout/              |           |\n",
      "|    ep_len_mean        | 200       |\n",
      "|    ep_rew_mean        | -1.31e+03 |\n",
      "| time/                 |           |\n",
      "|    fps                | 473       |\n",
      "|    iterations         | 200       |\n",
      "|    time_elapsed       | 2         |\n",
      "|    total_timesteps    | 1000      |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -1.41     |\n",
      "|    explained_variance | 0.0491    |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 199       |\n",
      "|    policy_loss        | -18.8     |\n",
      "|    std                | 0.988     |\n",
      "|    value_loss         | 410       |\n",
      "-------------------------------------\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<stable_baselines3.a2c.a2c.A2C at 0x20ea78e17e0>"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from stable_baselines3 import A2C\n",
    "from stable_baselines3.common.monitor import Monitor\n",
    "from stable_baselines3.common.vec_env import DummyVecEnv\n",
    "\n",
    "#使用Monitor Wrapper,会在训练的过程中输出rollout/ep_len_mean和rollout/ep_rew_mean,就是增加些日志\n",
    "#gym升级到0.26以后失效了,可能是因为使用了自定义的wapper\n",
    "\n",
    "env = gym.make('Pendulum-v1')\n",
    "env = DummyVecEnv([lambda: Monitor(env)])\n",
    "\n",
    "A2C('MlpPolicy', env, verbose=1).learn(1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "zuIcbfv3g9dd"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial observation: [[ 0.00128163 -0.00704087 -0.00235103]]\n",
      "Observation after one step: [[-0.9634776 -0.2636338 -0.9998802]]\n",
      "Reward: [-10.]\n",
      "Done: [False]\n",
      "Info: [{'TimeLimit.truncated': False}]\n"
     ]
    }
   ],
   "source": [
    "from stable_baselines3.common.vec_env import VecNormalize, VecFrameStack\n",
    "\n",
    "env = gym.make('Pendulum-v1')\n",
    "\n",
    "# VecNormalize,他会对state和reward进行Normalize \n",
    "# Wrap the environment in DummyVecEnv\n",
    "env = DummyVecEnv([lambda: env])\n",
    "env = VecNormalize(env)\n",
    "\n",
    "# To test the environment, you can reset and take a random action\n",
    "obs = env.reset()\n",
    "print(\"Initial observation:\", obs)\n",
    "\n",
    "action = [env.action_space.sample()]\n",
    "obs, reward, done, info = env.step(action)\n",
    "print(\"Observation after one step:\", obs)\n",
    "print(\"Reward:\", reward)\n",
    "print(\"Done:\", done)\n",
    "print(\"Info:\", info)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial observation: [[ 0.05012321  1.3666942  -0.8533298   0.64414805]\n",
      " [-1.135614   -0.4585393  -0.48197085 -1.3396527 ]\n",
      " [ 1.0856429  -0.9081749   1.3351983   0.69545174]]\n",
      "Action: [1, 1, 1]\n",
      "Observation after one step: [[ 0.1146841   1.3794004  -0.86218894 -0.9493386 ]\n",
      " [-1.2199925   0.8087723  -0.52086323 -1.1722691 ]\n",
      " [ 1.0906862   0.6635519   1.3682022  -0.8542514 ]]\n",
      "Reward: [10. 10. 10.]\n",
      "Done: [False False False]\n",
      "Info: [{'TimeLimit.truncated': False}, {'TimeLimit.truncated': False}, {'TimeLimit.truncated': False}]\n"
     ]
    }
   ],
   "source": [
    "from stable_baselines3.common.vec_env import VecNormalize, VecFrameStack\n",
    "\n",
    "\n",
    "n_envs = 3\n",
    "# Use a lambda function to create the environments\n",
    "env = DummyVecEnv([lambda: gym.make('CartPole-v1') for _ in range(n_envs)])\n",
    "# VecNormalize,他会对state和reward进行Normalize \n",
    "# Wrap the environment in DummyVecEnv\n",
    "\n",
    "env = VecNormalize(env)\n",
    "\n",
    "# To test the environment, you can reset and take a random action\n",
    "obs = env.reset()\n",
    "print(\"Initial observation:\", obs)\n",
    "\n",
    "action = [env.action_space.sample() for _ in range(n_envs)]\n",
    "print(\"Action:\",action)\n",
    "obs, reward, done, info = env.step(action)\n",
    "print(\"Observation after one step:\", obs)\n",
    "print(\"Reward:\", reward)\n",
    "print(\"Done:\", done)\n",
    "print(\"Info:\", info)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cuda device\n",
      "-----------------------------\n",
      "| time/              |      |\n",
      "|    fps             | 2926 |\n",
      "|    iterations      | 1    |\n",
      "|    time_elapsed    | 2    |\n",
      "|    total_timesteps | 8192 |\n",
      "-----------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 1533        |\n",
      "|    iterations           | 2           |\n",
      "|    time_elapsed         | 10          |\n",
      "|    total_timesteps      | 16384       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.015060211 |\n",
      "|    clip_fraction        | 0.279       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.679      |\n",
      "|    explained_variance   | -0.133      |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.121       |\n",
      "|    n_updates            | 10          |\n",
      "|    policy_gradient_loss | -0.0234     |\n",
      "|    value_loss           | 0.325       |\n",
      "-----------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\13306\\anaconda3\\envs\\SB3\\lib\\site-packages\\stable_baselines3\\common\\vec_env\\base_vec_env.py:243: UserWarning: You tried to call render() but no `render_mode` was passed to the env constructor.\n",
      "  warnings.warn(\"You tried to call render() but no `render_mode` was passed to the env constructor.\")\n"
     ]
    }
   ],
   "source": [
    "import gymnasium as gym\n",
    "from stable_baselines3.common.vec_env import DummyVecEnv\n",
    "from stable_baselines3 import PPO\n",
    "\n",
    "# Define the number of environments\n",
    "n_envs = 4\n",
    "\n",
    "# Use a lambda function to create the environments\n",
    "envs = DummyVecEnv([lambda: gym.make('CartPole-v1') for _ in range(n_envs)])\n",
    "envs = VecNormalize(envs)\n",
    "\n",
    "\n",
    "# Initialize the PPO model\n",
    "model = PPO('MlpPolicy', envs, verbose=1)\n",
    "\n",
    "# Train the model\n",
    "model.learn(total_timesteps=10000)\n",
    "\n",
    "# Test the trained model\n",
    "obs = envs.reset()\n",
    "for _ in range(1000):\n",
    "    action, _states = model.predict(obs)\n",
    "    obs, rewards, dones, info = envs.step(action)\n",
    "    envs.render()\n",
    "\n",
    "# Close the environments\n",
    "envs.close()\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "include_colab_link": true,
   "name": "2_gym_wrappers_saving_loading.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
